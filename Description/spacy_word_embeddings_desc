# [Example 2: word embeddings in spacy](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/spacy_word_embeddings_excercise.ipynb)
# First i gonna preprocess text which is in json format, then train different model with it and finally make measurement and visualize results
# This will be preprocessing phase
* I start from checking if there is a class imbalance, but we have perfect class balance
* Next i encode my classes into numbers, i gonna need fully numerical data
* Then i load large model from spacy trained on english words
# And create function that will put the text into the doc, it is nlp object
* Next it iterate through the tokens, removing **stop words and punctuation**
* And it will fill the list with the rest base tokens
* I apply this function on my data and see that it simplified the text
# Next i will do vectorization of the text to make it full numerical
* I create this vectors in separate column and it will be create from (preprocessed_text column)
* After vectorization i can create (train and test sets) from the already numerical data
* So the (x will be vector column with access to values) and (y will be the label column)
* Next using numpy i gonna stack first X_train and next X_test, they are now 2D data, as we can see the shapes has changed
# Then i can train a couple of models and see the results
* I start with DecisionTreeClassifier, score is not so good
* We get average accuracy 70% and f1-score for (1 and 2 class are 0.68 and 0.72)
# Next i gonna try MultinomialNB, but first i need to scale my data into (0 - 1 range)
