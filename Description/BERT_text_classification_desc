# [Example 5: BERT text classification](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Text_Classification_BERT.ipynb)
# In this exercise i gonna be creating BERT model for text classification
* First i can see, that i have class imbalance in my dataset, ham category is much bigger
* I gonna make them equal to the lower class, so i take only finepart of ham emails and that concatenate both classes

![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/imbalance.png)
![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/concatenation.png)

![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/balance.png)

# And of course i convert ham and spam column into numerical form, for ham - 0, spam - 1

# Then i can create train and test sets, specifying that the number of sample must be equal for y (stratify = df balanced['spam']))
* And i download 2 BERT modules for preprocessing and encoding

# Next i create function in which i use these 2 modules, it will help me first to get preprocessed text and next to get embedding vector
* First i put preprocess module and sepecify a place for sentence, it will preprocess given sentence
* Then i apply encoder on preprocessed text and we can call the model and put sentences
* As an output we get these probability vectors from -1 to 1

![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/BERT_uncased.png)
![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/sentence_embedding_function.png)

# Now i gonna put inside my function some phrases and check their cosine similarity
* It measure similarity between two non-zero vectors, so we have all we want, we just need to supply vectors indexes
* We can see that similarity between **(banana and grapes is higher - 0.99)** and **(between banana and jeff bezos is lower - 0.84)**, so everything works fine
* We can see that certain groups of words are arranged close to the other, banana and grapes re much likely to appear in same text than banana and Jeff Bezos
* But sometimes unrelated words appear in same text on which model is trained, thats why similarity is much higher than we expect

![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/vectors_from_words.png)
![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/cosine_similarity1.png)
![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/cosine_similarity2.png)

# Now i gonna create neural network using BERT combined with keras layers
* We start from passing input layer, shape will be figure out manually, data type is string, so we are gonna pass as an input some text
* Then we pass our BERT layer which preprocess these given text
