#[Example 5: BERT text classification](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Text_Classification_BERT.ipynb)
# In this exercise i gonna be creating BERT model for text classification
* First i can see, that i have class imbalance in my dataset, ham category is much bigger
* I gonna make them equal to the lower class, so i take only finepart of ham emails and that concatenate both classes

![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/imbalance.png)
![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/concatenation.png)
![](https://github.com/JakubTabor/Language_model_Pipeline/blob/main/Images_BERT_model/balance.png)

# And of course i convert ham and spam column into numerical form, for ham - 0, spam - 1

# Then i can create train and test sets, specifying that the number of sample must be equal for y (stratify = df balanced['spam']))
* And i download 2 BERT modules for preprocessing and encoding
